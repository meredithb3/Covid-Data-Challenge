---
title: "COVID-19 DataFest"
author: "Meredith Brown, Matt Feder, Pouya Mohammadi"
date: "`r Sys.Date()`"
output: html_document
---

# Load Packages
```{r load-packages, message=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(usmap)
library(ggthemes)
library(skimr)
library(broom)
library(knitr) 
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Datasets and Descriptions
MATT PUT IN DATA INFO HERE (when corona cases stopped tracking, what is pov dataset)

For demographic data, download file called stco-mr2010-1.csv from this url: https://www2.census.gov/programs-surveys/popest/datasets/2010/modified-race-data-2010/.
The data dictionary is located at https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2000-2010/mr2010.pdf.

```{r load-data, cache = TRUE}
covidData <- read.csv("coronacounties.csv")
povertyData <- read.csv("PovertyEstimates.csv")
populationData <- usmap::countypop
populationData$fips <- as.integer(populationData$fips)
demoData1 <- read.csv("stco-mr2010-1.csv")
demoData2 <- read.csv("stco-mr2010_mt_wy.csv")
demoData <- rbind(demoData1, demoData2)
```


# Section 1: Introduction

### Exploratory Data Analysis
First, let's start by glimpsing all of our datasets.

```{r cov-data}
glimpse(covidData)

covidData %>%
  group_by(fips) %>%
  count()
```

At first glimpse, the COVID-19 dataset has 61,971 observations and 6 variables. However, after grouping by FIPS code, we see that the dataset contains data on 2,709 counties, with repeated observations representing cases on different dates. We do however notice that this data does not seem to be cumulative and we should frame our analyses with this in mind.

```{r pov-data}
glimpse(povertyData)
```

The Poverty dataset has 3,193 observations (counties, states, and the US) and 34 variables. Since a wealthy county with a high population may still have a higher number of total individuals in poverty than a highly impoverished county with an increidbly low population, we will use the percent in poverty feature when conducting our visual analyses.

```{r demoData-glimpse}
glimpse(demoData)
```

We notice that our data on demographics is split up by age group and has a large number of classifications for the race of an individual which is important for census considerations but would complicate our modelling and visualizations, so we will limit our data to exclude age and bundle many of the less frequent races.


```{r demoData-setup, message=FALSE}
white_people_by_county <- demoData %>% 
        filter(IMPRACE == 1, ORIGIN == 1) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_white = sum(RESPOP))
  
black_people_by_county <- demoData %>% 
        filter(IMPRACE == 2, ORIGIN == 1) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_black = sum(RESPOP))

asian_people_by_county <- demoData %>% 
        filter(IMPRACE == 4, ORIGIN == 1) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_asian = sum(RESPOP))
  
hispanic_people_by_county <- demoData %>% 
        filter(ORIGIN == 2) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_hispanic = sum(RESPOP))

other_by_county <- demoData %>% 
        filter(ORIGIN == 1, !(IMPRACE %in% c(1,2,4))) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_other = sum(RESPOP))

non_white_people_by_county <- demoData %>% 
        filter(IMPRACE !=1 | ORIGIN != 1) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_nonwhite = sum(RESPOP))

demographicsData <- full_join(full_join( full_join(white_people_by_county,
                                black_people_by_county),
                                full_join(hispanic_people_by_county,
                                asian_people_by_county)),
                                full_join(other_by_county,
                                non_white_people_by_county))

create_fips <- function(ste, cnty){
  if (cnty < 10){
    c <- paste("00",as.character(cnty),sep="")
  }
  else if (cnty<100){
    c <- paste("0",as.character(cnty),sep="")
  }
  else{
    c <- as.character(cnty)
  }
  
  if (ste<10){
    s <- paste("0",as.character(ste),sep="")
  }
  else{
    s <- as.character(ste)
  }
  return(as.integer(paste(s,c,sep="")[1]))
}

demographicsData['fips'] <- mapply(create_fips, demographicsData$STATE, 
                                    demographicsData$COUNTY)

demoDfPopulation <- demographicsData['num_white'] + demographicsData['num_nonwhite'] 

demographicsData['perc_white'] <- demographicsData$num_white/demoDfPopulation
demographicsData['perc_black'] <- demographicsData$num_black/demoDfPopulation
demographicsData['perc_hispanic'] <- demographicsData$num_hispanic/demoDfPopulation
demographicsData['perc_nonwhite'] <- demographicsData$num_nonwhite/demoDfPopulation

glimpse(demographicsData)
```


All of our datasets have a variation of the variable `fips`, which is the Federal Information Processing Standard [https://en.wikipedia.org/wiki/FIPS_county_code] that allows counties to be uniquely identified. 

Thus, the FIPS variable is a good variable by which to join these 3 datasets:

```{r join}
data <- merge(covidData, povertyData, by.x = 'fips', by.y = 'FIPStxt')
data <- merge(data, populationData, by.x = 'fips', by.y = 'fips')
data <- merge(data, demographicsData, by.x = 'fips', by.y = 'fips')
glimpse(data)
```

We will focus our analysis on the number of deaths resulting from COVID-19 for a few diferrent reasons. Testing varies greatly between states and counties. A region that has not done that much testing could appear to have a low number of cases but have the same number of deaths as a region that has done a lot of testing and thus has turned up a large number of less severe COVID-19 cases. Meanwhile, analyzing deaths has a lower correlation with the number of tests performed as cases that are severe enough to end with a mortality tend to receive professional medical care and, thus, a test at some point. (We realize that this is not always the case as regions that are under-resourced often do not have the capacity to tend to all individuals or even test incredibly sick individuals.) Additionally, more affluent individuals with a primary doctor have had easier access to testing in the United States, and thus, this would lend us to believe that the percent of cases in affluent regions would be inflated when compared to the percent of cases in non-affluent regions in the United States. 

Beginning our EDA, we will first look at the total number of deaths in the United States by state. 


```{r cases-plot, warning=FALSE}
county_deaths <- data %>%
  group_by(fips) %>%
  mutate(total_deaths = sum(deaths)) %>%
  mutate(deathsPC = total_deaths/pop_2015) %>%
  mutate(logDeathsPC = log(deathsPC)) %>%
  filter(date == "2020-04-15")

state_deaths <- county_deaths %>%
  group_by(state) %>%
  summarise(total_state_deaths = sum(total_deaths))

cases_map <- plot_usmap(data = state_deaths, values = "total_state_deaths", labels = TRUE, label_color = "gray", color = "red")

cases_map <- cases_map + 
              scale_fill_gradient(low = "white", high = "#CB454A", name = "Deaths") +
              labs(title = "Deaths by State before April 16, 2020") + 
              labs(subtitle = "Most deaths are concentrated in New York State.") + 
              theme(legend.position = "right")

cases_map
```

Need to add histograms for the response variable we choose, the predictors we choose, and plots for the response v. predictors relationships
(We can do this in the section following - response variable description and predictor descriptions)

```{r cases-plot-per-capita, warning=FALSE}
state_capita_deaths <- county_deaths %>%
  group_by(state) %>%
  summarise(state_deathPC = 1000*sum(total_deaths)/sum(pop_2015))

capita_deaths_map <- plot_usmap(data = state_capita_deaths, values = "state_deathPC", labels = TRUE, label_color = "gray", color = "red")

capita_deaths_map <- capita_deaths_map + 
              scale_fill_gradient(low = "white", high = "#CB454A", name = "Deaths Per 1000 People") +
              labs(title = "Deaths per 1000 People by State before April 16, 2020") + 
              theme(legend.position = "right")

capita_deaths_map
```


```{r deaths-plot-county, warning=FALSE}
county_deaths <- data %>%
  group_by(fips) %>%
  mutate(total_deaths = sum(deaths)) %>%
  mutate(deathsPC = total_deaths/pop_2015) %>%
  mutate(logDeathsPC = log(deathsPC)) %>%
  filter(date == "2020-04-15")

county_deaths %>%
  select(state, county.x, deathsPC) %>%
  arrange(desc(deathsPC)) %>%
  head(10)

county_deaths_map <- plot_usmap(regions = "counties", data = county_deaths, values = "total_deaths",
                                color = "black", size=0.05)

 
county_deaths_map <- county_deaths_map + 
              scale_fill_gradient(low = "white", high = "limegreen", name = "Deaths") +
              labs(title = "Deaths by County on April 15, 2020") + 
              theme(legend.position = "right")

county_deaths_map


county_deaths_map <- plot_usmap("counties", data = county_deaths, values = "deathsPC", color = "black", size=0.05)

county_deaths_map <- county_deaths_map + 
              scale_fill_gradient(low = "white", high = "limegreen", name = "Deaths per Capita") +
              labs(title = "Deaths per Capita by County on April 15, 2020") + 
              theme(legend.position = "right")

county_deaths_map

county_deaths_map <- plot_usmap("counties", data = county_deaths, values = "logDeathsPC", color = "black", size = 0.05)

county_deaths_map <- county_deaths_map + 
              scale_fill_gradient(low = "white", high = "limegreen", name = "log(Deaths per Capita)") +
              labs(title = "Log Deaths per Capita by County on April 15, 2020") + 
              theme(legend.position = "right")

county_deaths_map
```

```{r poverty-plot-county, warning=FALSE}
plot_poverty_data <- merge(populationData, povertyData, by.x = 'fips', by.y = 'FIPStxt')

county_cases_map <- plot_usmap("counties", data = plot_poverty_data, values = "PCTPOVALL_2018", color = "red", size=0.01)

county_cases_map <- county_cases_map + 
              scale_fill_gradient(low = "white", high = "blue", name = "Percent in Poverty") +
              labs(title = "Poverty Level by County (2018 values)") + 
              theme(legend.position = "right")

county_cases_map
```



```{r deaths-hist}
county_deaths %>%
  ggplot(mapping = aes(x = total_deaths)) +
  geom_histogram(binwidth = 100) + 
  labs(title = "Histogram of Number of Deaths per County", x = "Number of Deaths",
       y = "# Counties with X Deaths")
```

As we predicted, we see an incredible amount of right skewness in the distribution of deaths per county.

```{r deathsPC-hist}
county_deaths %>%
  ggplot(mapping = aes(x = deathsPC)) +
  geom_histogram(binwidth = 0.0005) + 
  labs(title = "Histogram of Number of Deaths per Capita by County", x = "Number of Deaths/Capita",
       y = "# Counties with X Deaths/Capita")
```

There is still a lot of right skewness in the histogram of the number of deaths per capita. 

```{r logcases-hist, warning = FALSE}
county_deaths %>%
  ggplot(mapping = aes(x = logDeathsPC)) +
  geom_histogram(binwidth = 0.38) + 
  labs(title = "Histogram of Number of log of Deaths per Capita by County", x = "log(Deaths/Capita)",
       y = "# Counties with log(Deaths/Capita)")
```

This historam of the log(Deaths/Capita) by county shows much more evidence of normailty than the graph of just cases. It would be a much better response variable.

Another possible option to analyze as a response would be the number of cases per capita of a county. This may work better as it would correlate significantly less with population as we have "accounted" for this already in the response.

We, however, see a much more even distribution using the log of deaths per capita. It is important to note that many counties appear as grey in this map; these counties have zero cases and thus the log of them returns the value of negative infinity causing this. 
The histogram of the log of cases per capita is very normal which would be perfect for analysis.

### Description of the Response Variable

The response variable for use in the model will be log of deaths per capita. As shown above, this version of the response variable deals with some of the skewness in deaths per capita. 

### Descriptions of the Regressors

The full model will predict from all variables other than those that identify the area which the data point represents, such as FIPS code, data, county, and state.


# Section 2: Regression Analysis 
### Generating Our Model

```{r modelgen}
county_deaths <- county_deaths %>%
  filter(logDeathsPC != -Inf)

county_deaths <- county_deaths %>%
  as.numeric(as.character(c(POVALL_2018, CI90LBAll_2018, CI90UBALL_2018, POV017_2018, CI90LB017_2018, CI90UB017_2018, POV517_2018, CI90LB517_2018, CI90UB517_2018, MEDHHINC_2018, CI90LBINC_2018, CI90UBINC_2018)))

full_model <- lm(logDeathsPC ~ cases + deaths + Rural.urban_Continuum_Code_2003 + Urban_Influence_Code_2003 +       Rural.urban_Continuum_Code_2013 + Urban_Influence_Code_2013 + as.numeric(POVALL_2018) + CI90LBAll_2018 +  CI90UBALL_2018 + PCTPOVALL_2018 + CI90LBALLP_2018 + CI90UBALLP_2018 + POV017_2018 + CI90LB017_2018 + CI90UB017_2018 + PCTPOV017_2018 + CI90LB017P_2018 + CI90UB017P_2018 + POV517_2018 + CI90LB517_2018 + CI90UB517_2018 + PCTPOV517_2018 + CI90LB517P_2018 + CI90UB517P_2018 + MEDHHINC_2018 + CI90LBINC_2018 + CI90UBINC_2018 + pop_2015 + num_white + num_black + num_hispanic + num_asian + num_other + num_nonwhite + perc_white + perc_black + perc_hispanic + perc_nonwhite + total_deaths + deathsPC, data = county_deaths)

kable(tidy(full_model, format = "markdown"), digits = 4)
```


```{r model_select}
gbmodel_aug <- lm(data = movies, log_gb_rate ~ runtime + star_rating + log_num_user_star +
              + log_num_user_reviews_imdb + num_critic_reviews_imdb + movie_rating +
                  release_month + release_year + Action + Biography + Animation + Horror + Drama + Crime + Comedy + Adventure + Family + Fantasy + SciFi + Musical + Romance + Thriller + Mystery + Western + History + War + Music + Sport)
kable(tidy(gbmodel_aug, format = "markdown"), digits = 4)
model <- step(gbmodel_aug, direction = "backward")
```

Final linear regression model

### Testing Our Assumptions

```{r final-model}
final <- lm(data = movies, log_gb_rate ~ runtime + star_rating + log_num_user_star + log_num_user_reviews_imdb + movie_rating + Action + Horror + Drama + Crime + Adventure + SciFi + Romance + Western + History + Sport)
kable(tidy(final, format = "markdown"), digits = 4)
```

Above is the output of our final model, following the use of backwards selection.

```{r resid-plots}
movies <- movies %>% 
  mutate(predicted = predict.lm(final), residuals = resid(final))

resid1 <- ggplot(data = movies, aes(x=predicted, y=residuals)) + 
  geom_point(size = 0.2) + 
  geom_hline(yintercept=0,color="red") +
  labs(title="Residuals vs. Predicted Values") +
  theme_bw()

resid2 <- ggplot(data = movies, aes(x = runtime, y = residuals)) + 
  geom_point(size = 0.2) + 
  geom_hline(yintercept=0,color="red") +
  labs(title="Residuals vs. Runtime") +
  theme_bw()

resid3 <- ggplot(data = movies, aes(x = star_rating, y = residuals)) + 
  geom_point(size = 0.2) + 
  geom_hline(yintercept=0,color="red") +
  labs(title="Residuals vs. Star Rating") +
  theme_bw()

resid4 <- ggplot(data = movies, aes(x = log_num_user_star, y = residuals)) + 
  geom_point(size = 0.2) + 
  geom_hline(yintercept=0,color="red") +
  labs(title="Residuals vs. Log of the Number of User Star Ratings") +
  theme_bw()

resid5 <- ggplot(data = movies, aes(x=log_num_user_reviews_imdb, y=residuals)) + 
  geom_point(size = 0.2) + 
  geom_hline(yintercept=0,color="red") +
  labs(title="Residuals vs. Log of the Number of User Reviews") +
  theme_bw()

resid6 <- ggplot(data=movies,aes(x=movie_rating,y=residuals)) + 
  geom_boxplot() + 
  geom_hline(yintercept=0,color="red") +
  labs(title="Residuals vs. Movie Rating",
       x = "Region", 
       y="Residuals") +
  theme_bw()

resid1
grid.arrange(resid2, resid3, resid4) 
grid.arrange(resid5, resid6)
```

```{r normality}
ggplot(movies, aes(residuals)) +
  geom_histogram(bins = 15) +
  labs(title="Frequency of Residuals",
       x = "Residuals",
       y = "Frequency") +
  theme_bw()


ggplot(movies,aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Normal QQ Plot of Residuals") +
  theme_bw()

```

### Multicollinearity
```{r multicolvif}
vif(final)
```

<!-- We see from our variance inflation factor calculation that there are no values greater than 10 (and thus indicate multicolinearity) except for factors within the movie_rating variable. This is alright as we sometimes expect factors of a single variable to correlate with each other. For the most part, we do not have evidence of multicollinearity and shall continue with our analysis.  -->

### Interaction Effects
```{r interaction, warning = FALSE}
interaction1 <- lm(data = movies, log_gb_rate ~ runtime + star_rating + log_num_user_star + log_num_user_reviews_imdb + movie_rating + star_rating*runtime + log_num_user_star*runtime + log_num_user_reviews_imdb*runtime + movie_rating*runtime)
kable(anova(final, interaction1, test = "Chisq"), format = "markdown")

interaction2 <- lm(data = movies, log_gb_rate ~ runtime + star_rating + log_num_user_star + log_num_user_reviews_imdb + movie_rating + star_rating*runtime + log_num_user_star*star_rating + log_num_user_reviews_imdb*star_rating + movie_rating*star_rating)
kable(anova(final, interaction2, test = "Chisq"), format = "markdown")
```

Because the p-value is larger than 0.05 for interactions 1 and 2, there seems to be no significant interaction effect with runtime and star_rating. We will continue with our selected model.

### Influential Points
We should assess any possible inflential points in our model to make sure that there are no points that have a disproportionally large effect on our overall model. We will do so by assessing the leverage of each point on the model.
```{r augmenting}
augmented_model <- augment(gbmodel_aug)

leverage_threshold <- 2*(5+1)/(nrow(augmented_model))

high_lev <- augmented_model %>%
  filter(.hat > leverage_threshold)

augmented_model <- augmented_model %>%
  mutate(obs_num = 1:nrow(augmented_model))

ggplot(data = augmented_model, aes(x = obs_num,y = .hat)) + 
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept = leverage_threshold,color = "red")+
  labs(x = "Observation Number",y = "Leverage",title = "Leverage") +
  geom_text(aes(label=ifelse(.hat > leverage_threshold, as.character(obs_num), "")), nudge_x = 4)
```

We see that there are many values above 2 times the average leverage of our model so we shall go on to analyze the cooks distance of these specific values to make sure that they are not truly overly influential.

```{r cooksdanalyze}
high_lev %>%
  filter(.cooksd > 1)
movies
```

We see that there are no observations involved in our model that have a cooks distance of greater than 1. Thus, we have no overly influential points and may continue with the rest of our analysis. 

### Interpreting The Coefficients

<!-- The coefficients for our model all relate to the response variable which is the log of the gross income for a movie divided by its budget. We will call this the success ratio through the rest of the interpretation.  -->

<!-- The coeffficient for the `log_num_user_star` variable is 0.3342, because this variable is a log as well as our response variable, this means that for every increase by a factor of x of stars from user reviews for movie, we expect the success ratio to increase by that same factor of x. -->




# Section 3: Conclusion

### General Commentary

### Discussion of Results

### Limitations and Ideas for Further Research 
