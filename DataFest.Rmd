---
title: "COVID-19 DataFest"
author: "Meredith Brown, Matt Feder, Pouya Mohammadi"
date: "`r Sys.Date()`"
output: html_document
---

# Load Packages
```{r load-packages, message=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(usmap)
library(ggthemes)
library(skimr)
library(broom)
library(knitr) 
library(rms)
library(stats)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Datasets and Descriptions
MATT PUT IN DATA INFO HERE (when corona cases stopped tracking, what is pov dataset)

For demographic data, download file called stco-mr2010-1.csv from this url: https://www2.census.gov/programs-surveys/popest/datasets/2010/modified-race-data-2010/.
The data dictionary is located at https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2000-2010/mr2010.pdf.

```{r load-data, cache = TRUE}
covidData <- read.csv("coronacounties.csv")
povertyData <- read.csv("PovertyEstimates.csv")
populationData <- usmap::countypop
populationData$fips <- as.integer(populationData$fips)
demoData1 <- read.csv("stco-mr2010-1.csv")
demoData2 <- read.csv("stco-mr2010_mt_wy.csv")
demoData <- rbind(demoData1, demoData2)
```


# Section 1: Introduction

### Exploratory Data Analysis
First, let's start by glimpsing all of our datasets.

```{r cov-data}
glimpse(covidData)

covidData %>%
  group_by(fips) %>%
  count()
```

At first glimpse, the COVID-19 dataset has 61,971 observations and 6 variables. However, after grouping by FIPS code, we see that the dataset contains data on 2,709 counties, with repeated observations representing cases on different dates. We do however notice that this data does not seem to be cumulative and we should frame our analyses with this in mind.

```{r pov-data}
glimpse(povertyData)
```

The Poverty dataset has 3,193 observations (counties, states, and the US) and 34 variables. Since a wealthy county with a high population may still have a higher number of total individuals in poverty than a highly impoverished county with an increidbly low population, we will use the percent in poverty feature when conducting our visual analyses.

```{r demoData-glimpse}
glimpse(demoData)
```

We notice that our data on demographics is split up by age group and has a large number of classifications for the race of an individual which is important for census considerations but would complicate our modelling and visualizations, so we will limit our data to exclude age and bundle many of the less frequent races.


```{r demoData-setup, message=FALSE}
white_people_by_county <- demoData %>% 
        filter(IMPRACE == 1, ORIGIN == 1) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_white = sum(RESPOP))
  
black_people_by_county <- demoData %>% 
        filter(IMPRACE == 2, ORIGIN == 1) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_black = sum(RESPOP))

asian_people_by_county <- demoData %>% 
        filter(IMPRACE == 4, ORIGIN == 1) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_asian = sum(RESPOP))
  
hispanic_people_by_county <- demoData %>% 
        filter(ORIGIN == 2) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_hispanic = sum(RESPOP))

other_by_county <- demoData %>% 
        filter(ORIGIN == 1, !(IMPRACE %in% c(1,2,4))) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_other = sum(RESPOP))

non_white_people_by_county <- demoData %>% 
        filter(IMPRACE !=1 | ORIGIN != 1) %>% 
        group_by(STATE, COUNTY) %>% 
        summarise(num_nonwhite = sum(RESPOP))

demographicsData <- full_join(full_join( full_join(white_people_by_county,
                                black_people_by_county),
                                full_join(hispanic_people_by_county,
                                asian_people_by_county)),
                                full_join(other_by_county,
                                non_white_people_by_county))

create_fips <- function(ste, cnty){
  if (cnty < 10){
    c <- paste("00",as.character(cnty),sep="")
  }
  else if (cnty<100){
    c <- paste("0",as.character(cnty),sep="")
  }
  else{
    c <- as.character(cnty)
  }
  
  if (ste<10){
    s <- paste("0",as.character(ste),sep="")
  }
  else{
    s <- as.character(ste)
  }
  return(as.integer(paste(s,c,sep="")[1]))
}

demographicsData['fips'] <- mapply(create_fips, demographicsData$STATE, 
                                    demographicsData$COUNTY)

demoDfPopulation <- demographicsData['num_white'] + demographicsData['num_nonwhite'] 

demographicsData['perc_white'] <- demographicsData$num_white/demoDfPopulation
demographicsData['perc_black'] <- demographicsData$num_black/demoDfPopulation
demographicsData['perc_hispanic'] <- demographicsData$num_hispanic/demoDfPopulation
demographicsData['perc_nonwhite'] <- demographicsData$num_nonwhite/demoDfPopulation
demographicsData['perc_asian'] <- demographicsData$num_asian/demoDfPopulation
demographicsData['perc_other'] <- demographicsData$num_other/demoDfPopulation

glimpse(demographicsData)
```


All of our datasets have a variation of the variable `fips`, which is the Federal Information Processing Standard [https://en.wikipedia.org/wiki/FIPS_county_code] that allows counties to be uniquely identified. 

Thus, the FIPS variable is a good variable by which to join these 3 datasets:

```{r join}
data <- merge(covidData, povertyData, by.x = 'fips', by.y = 'FIPStxt')
data <- merge(data, populationData, by.x = 'fips', by.y = 'fips')
data <- merge(data, demographicsData, by.x = 'fips', by.y = 'fips')
glimpse(data)
```

We will focus our analysis on the number of deaths resulting from COVID-19 for a few diferrent reasons. Testing varies greatly between states and counties. A region that has not done that much testing could appear to have a low number of cases but have the same number of deaths as a region that has done a lot of testing and thus has turned up a large number of less severe COVID-19 cases. Meanwhile, analyzing deaths has a lower correlation with the number of tests performed as cases that are severe enough to end with a mortality tend to receive professional medical care and, thus, a test at some point. (We realize that this is not always the case as regions that are under-resourced often do not have the capacity to tend to all individuals or even test incredibly sick individuals.) Additionally, more affluent individuals with a primary doctor have had easier access to testing in the United States, and thus, this would lend us to believe that the percent of cases in affluent regions would be inflated when compared to the percent of cases in non-affluent regions in the United States. 

Beginning our EDA, we will first look at the total number of deaths in the United States by state. 


```{r cases-plot, warning=FALSE}
county_deaths <- data %>%
  group_by(fips) %>%
  mutate(total_deaths = sum(deaths)) %>%
  mutate(deathsPC = total_deaths/pop_2015) %>%
  mutate(logDeathsPC = log(deathsPC)) %>%
  filter(date == "2020-04-15")

state_deaths <- county_deaths %>%
  group_by(state) %>%
  summarise(total_state_deaths = sum(total_deaths))

cases_map <- plot_usmap(data = state_deaths, values = "total_state_deaths", labels = TRUE, label_color = "gray", color = "red")

cases_map <- cases_map + 
              scale_fill_gradient(low = "white", high = "#CB454A", name = "Deaths") +
              labs(title = "Deaths by State before April 16, 2020") + 
              labs(subtitle = "Most deaths are concentrated in New York State.") + 
              theme(legend.position = "right")

cases_map
```

Need to add histograms for the response variable we choose, the predictors we choose, and plots for the response v. predictors relationships
(We can do this in the section following - response variable description and predictor descriptions)

```{r cases-plot-per-capita, warning=FALSE}
state_capita_deaths <- county_deaths %>%
  group_by(state) %>%
  summarise(state_deathPC = 1000*sum(total_deaths)/sum(pop_2015))

capita_deaths_map <- plot_usmap(data = state_capita_deaths, values = "state_deathPC", labels = TRUE, label_color = "gray", color = "red")

capita_deaths_map <- capita_deaths_map + 
              scale_fill_gradient(low = "white", high = "#CB454A", name = "Deaths Per 1000 People") +
              labs(title = "Deaths per 1000 People by State before April 16, 2020") + 
              theme(legend.position = "right")

capita_deaths_map
```


```{r deaths-plot-county, warning=FALSE}
county_deaths <- data %>%
  group_by(fips) %>%
  mutate(total_deaths = sum(deaths)) %>%
  mutate(deathsPC = total_deaths/pop_2015) %>%
  mutate(logDeathsPC = log(deathsPC)) %>%
  filter(date == "2020-04-15")

county_deaths %>%
  select(state, county.x, deathsPC) %>%
  arrange(desc(deathsPC)) %>%
  head(10)

county_deaths_map <- plot_usmap(regions = "counties", data = county_deaths, values = "total_deaths",
                                color = "black", size=0.05)

 
county_deaths_map <- county_deaths_map + 
              scale_fill_gradient(low = "white", high = "limegreen", name = "Deaths") +
              labs(title = "Deaths by County on April 15, 2020") + 
              theme(legend.position = "right")

county_deaths_map


county_deaths_map <- plot_usmap("counties", data = county_deaths, values = "deathsPC", color = "black", size=0.05)

county_deaths_map <- county_deaths_map + 
              scale_fill_gradient(low = "white", high = "limegreen", name = "Deaths per Capita") +
              labs(title = "Deaths per Capita by County on April 15, 2020") + 
              theme(legend.position = "right")

county_deaths_map

county_deaths_map <- plot_usmap("counties", data = county_deaths, values = "logDeathsPC", color = "black", size = 0.05)

county_deaths_map <- county_deaths_map + 
              scale_fill_gradient(low = "white", high = "limegreen", name = "log(Deaths per Capita)") +
              labs(title = "Log Deaths per Capita by County on April 15, 2020") + 
              theme(legend.position = "right")

county_deaths_map
```

```{r poverty-plot-county, warning=FALSE}
plot_poverty_data <- merge(populationData, povertyData, by.x = 'fips', by.y = 'FIPStxt')

county_cases_map <- plot_usmap("counties", data = plot_poverty_data, values = "PCTPOVALL_2018", color = "red", size=0.01)

county_cases_map <- county_cases_map + 
              scale_fill_gradient(low = "white", high = "blue", name = "Percent in Poverty") +
              labs(title = "Poverty Level by County (2018 values)") + 
              theme(legend.position = "right")

county_cases_map
```



```{r deaths-hist}
county_deaths %>%
  ggplot(mapping = aes(x = total_deaths)) +
  geom_histogram(binwidth = 100) + 
  labs(title = "Histogram of Number of Deaths per County", x = "Number of Deaths",
       y = "# Counties with X Deaths")
```

As we predicted, we see an incredible amount of right skewness in the distribution of deaths per county.

```{r deathsPC-hist}
county_deaths %>%
  ggplot(mapping = aes(x = deathsPC)) +
  geom_histogram(binwidth = 0.0005) + 
  labs(title = "Histogram of Number of Deaths per Capita by County", x = "Number of Deaths/Capita",
       y = "# Counties with X Deaths/Capita")
```

There is still a lot of right skewness in the histogram of the number of deaths per capita. 

```{r logcases-hist, warning = FALSE}
county_deaths %>%
  ggplot(mapping = aes(x = logDeathsPC)) +
  geom_histogram(binwidth = 0.38) + 
  labs(title = "Histogram of Number of log of Deaths per Capita by County", x = "log(Deaths/Capita)",
       y = "# Counties with log(Deaths/Capita)")
```

This historam of the log(Deaths/Capita) by county shows much more evidence of normailty than the graph of just cases. It would be a much better response variable.

Another possible option to analyze as a response would be the number of cases per capita of a county. This may work better as it would correlate significantly less with population as we have "accounted" for this already in the response.

We, however, see a much more even distribution using the log of deaths per capita. It is important to note that many counties appear as grey in this map; these counties have zero cases and thus the log of them returns the value of negative infinity causing this. 
The histogram of the log of cases per capita is very normal which would be perfect for analysis.

### Description of the Response Variable

The response variable for use in the model will be log of deaths per capita. As shown above, this version of the response variable deals with some of the skewness in deaths per capita. 

### Descriptions of the Regressors

The full model will predict from all variables other than those that identify the area which the data point represents, such as FIPS code, data, county, and state.

# Section 2: Regression Analysis 
### Generating Our Model

```{r modelgen}
county_deaths <- county_deaths %>%
  filter(logDeathsPC != -Inf)

county_deaths$POVALL_2018 <- as.numeric(gsub(",", "", county_deaths$POVALL_2018))
county_deaths$POV017_2018 <- as.numeric(gsub(",", "", county_deaths$POV017_2018))
county_deaths$POV517_2018 <- as.numeric(gsub(",", "", county_deaths$POV517_2018))
county_deaths$MEDHHINC_2018 <- as.numeric(gsub(",", "", county_deaths$MEDHHINC_2018))

county_deaths <- county_deaths %>%
  filter(!is.na(Rural.urban_Continuum_Code_2003))
county_deaths <- county_deaths %>%
  filter(!is.na(Urban_Influence_Code_2003))

county_deaths$Rural.urban_Continuum_Code_2013 <- as.factor(county_deaths$Rural.urban_Continuum_Code_2013)

full_model <- lm(logDeathsPC ~ cases + 
                 Rural.urban_Continuum_Code_2013 + 
                 PCTPOVALL_2018 + 
                 + MEDHHINC_2018 + pop_2015 + perc_asian +
                 perc_other + perc_white + perc_black +
                   perc_hispanic + perc_nonwhite, data = county_deaths)

kable(tidy(full_model, format = "markdown"), digits = 4)
```

```{r model_select}
model <- step(full_model, direction = "backward", trace = FALSE)
kable(tidy(model, format = "markdown"), digits = 4)
```

```{r final-model}
final_model <- lm(logDeathsPC ~ cases + Rural.urban_Continuum_Code_2013 + 
                    PCTPOVALL_2018 +
                    pop_2015 + perc_white + perc_black, data = county_deaths)

kable(tidy(final_model, format = "markdown"), digits = 7)
```

Above is the output of our final model, following the use of backwards selection.

### Distributions of Regressors

```{r cases}
ggplot(data = county_deaths, aes(x = cases)) +
  geom_histogram(color = "Black", fill = "Light Blue", bins = 20) + 
  labs(title = "Distribution of Cases", x = "Cases", y = "Count")
```

```{r rural-urban-code}
ggplot(data = county_deaths, aes(x = as.factor(Rural.urban_Continuum_Code_2013))) +
  geom_boxplot(color = "Black", fill = "Light Blue") + 
  labs(title = "Distribution of Rural-Urban Continuum Code", x = "Code", y = "Count")
```

```{r pct-pov-0-17}
ggplot(data = county_deaths, aes(x = PCTPOVALL_2018)) +
  geom_histogram(color = "Black", fill = "Light Blue", bins = 20) + 
  labs(title = "Distribution of Percent Poverty", x = "Percent in Poverty", y = "Count")
```


```{r population-2015}
ggplot(data = county_deaths, aes(x = pop_2015)) +
  geom_histogram(color = "Black", fill = "Light Blue", bins = 20) + 
  labs(title = "Distribution of Population (2015)", x = "Population (2015)", y = "Count")
```

```{r perc-white}
ggplot(data = county_deaths, aes(x = perc_white)) +
  geom_histogram(color = "Black", fill = "Light Blue", bins = 20) + 
  labs(title = "Distribution of White Percent of Population ", x = "White Percent of Population", 
       y = "Count")
```

```{r perc-black}
ggplot(data = county_deaths, aes(x = perc_black)) +
  geom_histogram(color = "Black", fill = "Light Blue", bins = 20) + 
  labs(title = "Distribution of Black Percent of Population ", x = "Black Percent of Population", 
       y = "Count")
```

The distributions for `cases`, `pop_2015`, `perc_white`, and `perc_black` all appear to be extremely skewed, so they do not satisfy the normality condition. To correct this, we will try taking the log of each of these variables.

```{r log-cases}
ggplot(data = county_deaths, aes(x = log(cases))) +
  geom_histogram(color = "Black", fill = "Light Blue", bins = 20) + 
  labs(title = "Distribution of Log Cases", x = "Log Cases", y = "Count")
```

```{r log-population}
ggplot(data = county_deaths, aes(x = log(pop_2015))) +
  geom_histogram(color = "Black", fill = "Light Blue", bins = 20) + 
  labs(title = "Distribution of Log Population (2015)", x = "Log Population (2015)", y = "Count")
```

```{r log-perc-white}
ggplot(data = county_deaths, aes(x = log(perc_white))) +
  geom_histogram(color = "Black", fill = "Light Blue", bins = 20) + 
  labs(title = "Distribution of Log White Percent of Population ", x = "Log White Percent of Population", 
       y = "Count")
```

```{r perc-black}
ggplot(data = county_deaths, aes(x = log(perc_black))) +
  geom_histogram(color = "Black", fill = "Light Blue", bins = 20) + 
  labs(title = "Distribution of Log Black Percent of Population ", x = "Log Black Percent of Population", 
       y = "Count")
```

After these transformations, the `cases`, `pop_2015`, and `num_asian` variables are much more normally distributed. The `perc_white` and `perc_black` variables are still skewed when log transformed, so we will work with the untransformed versions of them, and mark their failure of this condition in our conclusion.

After performing the transformations on 3 of the variables, the final model needs to be updated to reflect this:

```{r final-model-2, warning=FALSE, message=FALSE}
county_deaths <- county_deaths %>%
  mutate(logCases = log(cases),
         log_pop_2015 = log(pop_2015))

final <- lm(logDeathsPC ~ logCases + Rural.urban_Continuum_Code_2013 + 
                    PCTPOVALL_2018 + log_pop_2015 + perc_white + perc_black, data = county_deaths)

kable(tidy(final, format = "markdown"), digits = 7)
kable(tidy(final, format = "markdown", exponentiate = TRUE), digits = 7)
```

### Interpreting The Coefficients

Now that we have determined that our model is satisfactory, we can begin to provide an interpretation of the model. We see, unsurprisingly, that the number of cases has a high correlation with the number of deaths per capita in a county. However, looking at the p-values of the model coefficients we also see that there is strong evidence that there is a correlation between certain urban continuum codes, or how large and urbanized a county is, and how many deaths per capita there are in the county. Interestingly, the two codes with the strongest evidence (or the lowest p-value) to be correlated with the deaths per capita in a county are for counties with a code 9 or "less than 2,500 urban population, not adjacent to a metro area" and for counties with a code 3 or "counties in metro areas of fewer than 250,000 population."

A larger urban continuum code indicates that a county is less urban and less populated.

We will also interpret the model coefficients from our model. We expect that as the log number of cases increases by 1, we expect the number of deaths per capita in the county to increase by 2.255 on average. Additionally, we expect that as the log population increases by 1, we expect the the number of deaths per capita to increase by 1.3779. Next, we expect that as the percentage of white inhabitants in a county increases by 1 percent, the deaths per capita in the county will increase by 0.013 on average. Meanwhile, as the percentage of black inhabitants in a county increases by 1 percent, the deaths per capita in the county will increase by 0.0077 on average. If the percentage of individuals in poverty in a county increases by 1 percent in a county, we expect the deaths per capita in the county to increase by 0.0101 on average. In the following paragraph, we will provide interpretation for the coefficients corresponding to the rural urban continuum codes generated by the model.

A key of all of the rural urban continuum codes can be found at https://seer.cancer.gov/seerstat/variables/countyattribs/ruralurban.html.
In general, as the code number increases, a county is smaller and less urban.

The baseline rural urban continuum code for our analysis is a code 1. If a county has a rural urban continuum code of 2, we expect there to be 0.8997374 deaths per capita more than counties with a baseline code on average. If a county has a rural urban continuum code of 3, we expect there to be 0.7460708 deaths per capita more than counties with a baseline code on average. If a county has a rural urban continuum code of 4, we expect there to be 0.8093420 deaths per capita more than counties with a baseline code on average. If a county has a rural urban continuum code of 5, we expect there to be 0.8770798 deaths per capita more than counties with a baseline code on average. If a county has a rural urban continuum code of 6, we expect there to be 0.9905803 deaths per capita more than counties with a baseline code on average. If a county has a rural urban continuum code of 7, we expect there to be 1.1904460 deaths per capita more than counties with a baseline code on average. If a county has a rural urban continuum code of 8, we expect there to be 1.4139911 deaths per capita more than counties with a baseline code on average. If a county has a rural urban continuum code of 9, we expect there to be 1.8102279 deaths per capita more than counties with a baseline code on average. This trend seems to indicate that as a county becomes less urban, the average deaths per capita seems to increase.

#### Interaction Effects
```{r interaction, warning = FALSE}

```


### Testing Our Assumptions

Now, let's check our model assumptions before moving on to any inferential analysis. We start by checking the linearity assumption.

#### Linearity

We can check the linearity assumption by graphing the response variable against all the predictor variables within our model. 

```{r logdeathsPC-logcases}
ggplot(data = county_deaths, aes(x = logCases, y = logDeathsPC)) +
  geom_point(color = "black") +
  labs(title = "Log Deaths/Capita vs Log Cases", y = "Log Deaths/Capita", x = "Log Cases")
```

From the graph above, we can see that there is a slightly positive association between log deaths per capita and the log number of cases. We would expect this, as more cases would likely contribute to more deaths per capita. There are no obvious concerns with linearity here, although the data is sort of spread out in a slight backwards fan. 

```{r logdeathsPC-code}
ggplot(data = county_deaths, aes(x = as.factor(Rural.urban_Continuum_Code_2013), y = logDeathsPC)) +
  geom_boxplot(color = "black", fill = "light blue") +
  labs(title = "Log Deaths/Capita vs Rural-Urban Continuum Code", y = "Log Deaths/Capita", 
       x = "Rural-Urban Continuum Code")
```

From the boxplots above, there appears to be a slight positive linear assoication in log deaths per capita amongst the different rural-urban continuum codes. Since this is a categorical variable, it becomes a bit harder for us to distinguish any glaring concerns with linearity.

```{r logdeathsPC-pct-pov-0-17}
ggplot(data = county_deaths, aes(x = PCTPOVALL_2018, y = logDeathsPC)) +
  geom_point(color = "black") +
  labs(title = "Log Deaths/Capita vs Percent Poverty", y = "Log Deaths/Capita", 
       x = "Percent in Poverty")
```

From the graph above, we can see that there is a very slightly positive association between log deaths per capita and the percent poverty in 2018. We would expect this, as a higher percent of people in poverty would likely contribute to more deaths per capita. There are no obvious concerns with linearity here, although the linear association is very weak. 

```{r logdeathsPC-logpopulation}
ggplot(data = county_deaths, aes(x = log_pop_2015, y = logDeathsPC)) +
  geom_point(color = "black") +
  labs(title = "Log Deaths/Capita vs Log Population (2015)", y = "Log Deaths/Capita", 
       x = "Log Population (2015)")
```

From the graph above, we can see that there is a negative linear association between log deaths per capita and the log of population in 2015. This is interesting because it implies that an increase in log population correlates with a decrease in log deaths per capita. There are no obvious concerns with linearity here. 

```{r logdeathsPC-perc_white}
ggplot(data = county_deaths, aes(x = perc_white, y = logDeathsPC)) +
  geom_point(color = "black") +
  labs(title = "Log Deaths/Capita vs White Percent of Population", y = "Log Deaths/Capita", 
       x = "White Percent of Population")
```  

```{r logdeathsPC-perc_black}
ggplot(data = county_deaths, aes(x = perc_black, y = logDeathsPC)) +
  geom_point(color = "black") +
  labs(title = "Log Deaths/Capita vs Black Percent of Population", y = "Log Deaths/Capita", 
       x = "Black Percent of Population")
```  

Both the above graphs of specific percents of the population appear to violate the linearity assumption. This makes sense, as we saw in their distributions in the Exploratory Data Analysis that their initial distributions were skewed and not fixed by a transformation. Thus, we can proceed with this analysis with caution.

#### Constant Variance

Next, we can check the constant variance assumption by looking at the plot of the residuals of each predictor variable and the response variable. For this assumption to be satisfied, the regression variance must be the around the same for each predictor variable (randomly scattered points around y=0 line in residual plot).

```{r ridemodelresid}
county_deaths$predicted = predict.lm(final)
county_deaths$resid = residuals(final)

```

```{r predict-resid}
ggplot(data = county_deaths, mapping = aes(x = predicted, y = resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0, color = "red") + 
  labs(title = "Residuals vs. Predicted Log Deaths/Capita",
       x = "Log(Price)",
       y = "Residual")
```

#### Normality

Next, we can check the normality assumption by plotting the histogram of the residuals and the normal QQ plot of the residuals to check for any discrepancies and departures from normality.

```{r residhist}
ggplot(data = county_deaths, mapping = aes(x = resid)) +   
  geom_histogram(color = "Black", fill = "Light Blue", binwidth = 0.05) + 
  labs(title = "Distribution of Residuals",
       x = "Residuals",
       y = "Frequency")
```

The distribution of residuals from our model appears to be mostly normal, however there is a slight left-skewness Therefore, we would like to investigate the normality of the model a bit further by looking at a QQ plot.

```{r residqq}
ggplot(data = county_deaths, mapping = aes(sample = resid)) +  
  stat_qq() + 
  stat_qq_line() +
  labs(title = "Normal QQ Plot of Residuals") 
```

From the two graphs above, we can see that the normality assumption is mostly satisfied. While there does appear to be some deviation from the normal QQ plot towards the edges of the graphs, we believe that this is enough to satisfy the normality for this design, so we will process with our testing.

#### Independence

We do have a few concerns about the independence of our observations, mainly we believe that there may be some correlation between neighboring counties. Since the virus must be transmitted from person to person under our current knowledge, we believe that neighboring counties, which are divided not by a physical border, but just by a cartographic line, may have a highly correlated number of deaths per capita, as well as similar characteristics or features. However, we will proceed with caution in our analysis. 

### Checking Model Diagnostics

```{r aug-model}
final_output <- augment(final) %>%
  mutate(obs_num = row_number())

head(final_output, 5)
```

The model diagnostics for MLR are checking leverage, standardized residuals, Cook's distance, and VIF values. 

#### Multicollinearity
```{r multicolvif}
vif(final)
```


#### Influential Points

We should assess any possible inflential points in our model to make sure that there are no points that have a disproportionally large effect on our overall model. We will do so by assessing the leverage of each point on the model.

```{r augmenting}
augmented_model <- augment(final)

leverage_threshold <- 2*(5+1)/(nrow(augmented_model))

high_lev <- augmented_model %>%
  filter(.hat > leverage_threshold)

augmented_model <- augmented_model %>%
  mutate(obs_num = 1:nrow(augmented_model))

ggplot(data = augmented_model, aes(x = obs_num,y = .hat)) + 
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept = leverage_threshold,color = "red")+
  labs(x = "Observation Number",y = "Leverage",title = "Leverage") +
  geom_text(aes(label=ifelse(.hat > leverage_threshold, as.character(obs_num), "")), nudge_x = 4)
```

We see that there are many values above 2 times the average leverage of our model so we shall go on to analyze the cooks distance of these specific values to make sure that they are not truly overly influential.

```{r cooksdanalyze}
high_lev %>%
  filter(.cooksd > 1)
```

We see that there are no observations involved in our model that have a cooks distance of greater than 1. Thus, we have no overly influential points and may continue with the rest of our analysis. 

# Section 3: Conclusion

### General Commentary

### Discussion of Results

### Limitations and Ideas for Further Research 
